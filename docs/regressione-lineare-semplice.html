<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capitolo 3 Regressione lineare semplice | Dispense modulo univariata</title>
  <meta name="description" content="Capitolo 3 Regressione lineare semplice | Dispense modulo univariata" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capitolo 3 Regressione lineare semplice | Dispense modulo univariata" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="Immagini/Modunivar.jpg" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capitolo 3 Regressione lineare semplice | Dispense modulo univariata" />
  
  
  <meta name="twitter:image" content="Immagini/Modunivar.jpg" />

<meta name="author" content="Giorgio Marrubini e Camillo Melzi" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="Immagini/modunivar.ico" type="image/x-icon" />
<link rel="prev" href="statistica-inferenziale.html"/>
<link rel="next" href="analisi-della-varianza.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MODUNIVAR</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="glossario.html"><a href="glossario.html"><i class="fa fa-check"></i>Glossario minimo</a></li>
<li class="chapter" data-level="1" data-path="statistica-descrittiva.html"><a href="statistica-descrittiva.html"><i class="fa fa-check"></i><b>1</b> Statistica descrittiva</a></li>
<li class="chapter" data-level="2" data-path="statistica-inferenziale.html"><a href="statistica-inferenziale.html"><i class="fa fa-check"></i><b>2</b> Statistica inferenziale</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistica-inferenziale.html"><a href="statistica-inferenziale.html#stime-della-media"><i class="fa fa-check"></i><b>2.1</b> Stime della media</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistica-inferenziale.html"><a href="statistica-inferenziale.html#una-popolazione-gaussiana"><i class="fa fa-check"></i><b>2.1.1</b> Una popolazione gaussiana</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistica-inferenziale.html"><a href="statistica-inferenziale.html#due-popolazioni-gaussiane-campioni-accoppiati"><i class="fa fa-check"></i><b>2.1.2</b> Due popolazioni gaussiane: campioni accoppiati</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistica-inferenziale.html"><a href="statistica-inferenziale.html#due-popolazioni-gaussiane-campioni-indipendenti"><i class="fa fa-check"></i><b>2.1.3</b> Due popolazioni gaussiane: campioni indipendenti</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regressione-lineare-semplice.html"><a href="regressione-lineare-semplice.html"><i class="fa fa-check"></i><b>3</b> Regressione lineare semplice</a>
<ul>
<li class="chapter" data-level="3.1" data-path="regressione-lineare-semplice.html"><a href="regressione-lineare-semplice.html#stima-dei-parametri-beta_0-e-beta_1"><i class="fa fa-check"></i><b>3.1</b> Stima dei parametri <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="regressione-lineare-semplice.html"><a href="regressione-lineare-semplice.html#significatività-dei-coefficienti"><i class="fa fa-check"></i><b>3.2</b> Significatività dei coefficienti</a></li>
<li class="chapter" data-level="3.3" data-path="regressione-lineare-semplice.html"><a href="regressione-lineare-semplice.html#previsione-del-modello"><i class="fa fa-check"></i><b>3.3</b> Previsione del modello</a></li>
<li class="chapter" data-level="3.4" data-path="regressione-lineare-semplice.html"><a href="regressione-lineare-semplice.html#verifica-delle-ipotesi-1"><i class="fa fa-check"></i><b>3.4</b> Verifica delle ipotesi</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analisi-della-varianza.html"><a href="analisi-della-varianza.html"><i class="fa fa-check"></i><b>4</b> Analisi della Varianza</a>
<ul>
<li class="chapter" data-level="4.1" data-path="analisi-della-varianza.html"><a href="analisi-della-varianza.html#analisi-della-varianza-a-un-fattore"><i class="fa fa-check"></i><b>4.1</b> Analisi della varianza a un fattore</a></li>
<li class="chapter" data-level="4.2" data-path="analisi-della-varianza.html"><a href="analisi-della-varianza.html#analisi-della-varianza-a-due-fattori"><i class="fa fa-check"></i><b>4.2</b> Analisi della varianza a due fattori</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Dispense modulo univariata</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regressione-lineare-semplice" class="section level1" number="3">
<h1><span class="header-section-number">Capitolo 3</span> Regressione lineare semplice</h1>
<p>Ricordiamo brevemente l’equazione di una retta su un piano di coordinate <span class="math inline">\((x,y)\)</span>:
<span class="math display">\[
y=\beta_0+\beta_1x
\]</span>
il cui grafico è dato da</p>
<p><img src="Immagini/Regressione/01_graf_retta.png" width="50%" style="display: block; margin: auto;" /></p>
<p>dove <span class="math inline">\(\beta_0\)</span> è l’intercetta, ossia il valore della <span class="math inline">\(y\)</span> per <span class="math inline">\(x=0\)</span>, e <span class="math inline">\(\beta_1\)</span> è la pendenza della retta e indica di quanto cresce (o decresce) la <span class="math inline">\(y\)</span> all’aumentare di un’unità nella <span class="math inline">\(x\)</span>.</p>
<p>Supponiamo ora di dover costruire la retta di taratura (i.e. determinare i parametri <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span>) di uno strumento per la determinazione delle proteine totali in colture cellulari. L’esperimento consiste nel leggere l’assorbanza misurata <span class="math inline">\(y\)</span> da uno spettofotometro per concentrazioni note <span class="math inline">\(x\)</span>. I dati sperimentali sono indicati nella seguente tabella</p>
<pre><code>##   Concentrazione Assorbanza
## 1            100      2.298
## 2             80      2.018
## 3             50      1.618
## 4             40      1.383
## 5             20      0.882</code></pre>
<p>I 5 risultati sperimentali della tabella sono un campione della popolazione di tutti gli esperimenti teoricamente possibili, e a partire da questo campione vogliamo stimare i parametri <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span>.</p>
<p>I 5 risultati sperimentali li rappresentiamo nel piano <span class="math inline">\((concentrazione,assorbanza)\)</span></p>
<p><img src="Immagini/Regressione/02_graf_dati.png" width="50%" style="display: block; margin: auto;" /></p>
<p>La retta tratteggiata è la retta di taratura i cui parametri vogliamo stimare a partire dal campione delle 5 misure sperimentali date.</p>
<p>Ragionando in maniera simile a quanto fatto nel capitolo <em>Statistica inferenziale</em> per la media, possiamo supporre che la misura sperimentale <span class="math inline">\(y\)</span> per un livello di concentrazione <span class="math inline">\(x\)</span> fissato differisca dal risultato terorico “vero” <span class="math inline">\(\beta_0+\beta_1x\)</span> per un errore sperimentale <span class="math inline">\(\epsilon\)</span> puramente casuale
<span class="math display">\[
y=\beta_0+\beta_1x + \epsilon
\]</span>
e che l’errore sperimentale <span class="math inline">\(\epsilon\)</span> sia distribuito come una normale di media 0 e varianza <span class="math inline">\(\sigma^2\)</span>
<span class="math display">\[
\epsilon \sim N(0,\sigma^2)
\]</span>
Si noti che la varianza <span class="math inline">\(\sigma^2\)</span> è ipotizzata sempre uguale per tutte le misure (ipotesi di omoschedasticità).<br />
N.B. Ciò significa, sul piano pratico, che le 5 misure dei valori di y abbiano uguale precisione (in senso strettamente analitico, la precisione si rappresenta con il coefficiente di variazione che è la deviazione standard relativa%).</p>
<p>Altra importante ipotesi è che le misure siano indipendenti le une dalle altre, ossia che il risultato di ogni misura (ad es. <span class="math inline">\(y_1\)</span>)
non è influenzato in alcun modo da una misura precedente o successiva (ad es. <span class="math inline">\(y_2\)</span>).</p>
<div id="stima-dei-parametri-beta_0-e-beta_1" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Stima dei parametri <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span></h2>
<p>Data una retta qualsiasi del piano <span class="math inline">\(y=b_0+b_1x\)</span>, indichiamo con <span class="math inline">\(e_i\)</span> la differenza tra i 5 valori misurati <span class="math inline">\(y_i\)</span> e i 5 valori <span class="math inline">\(b_0+b_1x_i\)</span>
<span class="math display">\[
e_i=y_i-(b_0+b_1x_i)
\]</span></p>
<p>indicate con le frecce rosse in figura</p>
<p><img src="Immagini/Regressione/04_min_sq.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Si può dimostrare, (v. <strong>metodo dei minimi quadrati</strong> nel Glossario), che esiste una unica retta <span class="math inline">\(y=\hat{\beta_0}+\hat{\beta_1}x\)</span> che minimizza la somma dei quadrati <span class="math inline">\(\sum_{i=1}^m e_i^2\)</span> (<span class="math inline">\(m=5\)</span> nel nostro esempio) e che i valori <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> sono dati da
<span class="math display">\[\begin{eqnarray*}
\hat{\beta_1}&amp;=&amp;\frac{\sum_{i=1}^m (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^m(x_i-\bar{x})^2} \\
\hat{\beta_0}&amp;=&amp;\bar{y}-\hat{\beta}_1\bar{x}.
\end{eqnarray*}\]</span></p>
<p>I valori <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> dipendono dal campione di 5 misure, un nuovo campione porta a valori diversi. Sono variabili aleatorie.</p>
<p>Si verifica facilmente che <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> sono una stima puntuale dei parametri <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span>, cioè ripetendo un gran numero di esperimenti “in media” <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> tendono a coincidere con il valore «vero» <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span>.<br />
N.B. Questo concetto, si rappresenta formalmente con la media, o speranza matematica di una variabile casuale (ad es. <span class="math inline">\(\beta_0\)</span>), e si indica con</p>
<p><span class="math display">\[
\mathbb{E}(\hat{\beta_0})=\beta_0 \quad \rm{e} \quad \mathbb{E}(\hat{\beta_1})=\beta_1
\]</span>
in cui la notazione
<span class="math display">\[
\mathbb{E}()
\]</span>
deriva da <em>expected</em> o <em>expectation</em> in inglese o dal francese <em>espérance</em>, e formalizza il concetto di valore medio di un fenomeno aleatorio.</p>
<p>E’ nota anche la precisione di questi stimatori, infatti si può dimostrare che</p>
<p><span class="math display">\[\begin{eqnarray*}
\rm{Var}(\hat{\beta_0})&amp;=&amp;[\frac{1}{m}+\frac{\bar{x}^2}{\sum_{i=1}^m(x_i-\bar{x})^2}]\sigma^2\\
\rm{Var}(\hat{\beta_1})&amp;=&amp;\frac{\sigma^2}{\sum_{i=1}^m(x_i-\bar{x})^2} .
\end{eqnarray*}\]</span></p>
<p>Si noti che le varianze degli stimatori non dipendono dai risultati sperimentali <span class="math inline">\(y_i\)</span>, sono anche chiamate «errore standard» degli stimatori (Excel). Sono determinate a priori – vale a dire che dipendono solo dal piano sperimentale.
Inoltre si noti che sono inversamente proporzionali a <span class="math inline">\(\sum_{i=1}^m(x_i-\bar{x})^2\)</span>, cioè alla misura della “dispersione” delle <span class="math inline">\(x\)</span>.</p>
<p>Le stime puntuali, una volta caricato il dataset, si ottengono nel menù Regressione lineare/semplice</p>
<p><img src="Immagini/Regressione/05_stima_pt.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="significatività-dei-coefficienti" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Significatività dei coefficienti</h2>
<p>Per fare inferenza a partire dalle 5 misure campionarie, abbiamo bisogno di conoscere la distribuzione di probabilità degli stimatori <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span>. Abbiamo innanzitutto bisogno di stimare la varianza <span class="math inline">\(\sigma^2\)</span>. Una stima puntuale è data da:
<span class="math display">\[
s^2=\frac{\sum_{i=1}^m[y_i-(\hat{\beta_0} + \hat{\beta_1}x_1)]^2}{m-2}
\]</span>
Si può dimostrare che</p>
<p><span class="math display">\[\begin{eqnarray*}
\frac{\hat{\beta_0}-\beta_0}{s\sqrt{h}}&amp;\sim&amp;t(m-2)\\
\frac{\hat{\beta_1}-\beta_1}{s\sqrt{1/S^2_{x}}}&amp;\sim&amp;t(m-2),
\end{eqnarray*}\]</span>
dove <span class="math inline">\(h=\frac{1}{m}+\frac{\bar{x}^2}{S^2_{x}}\)</span> e <span class="math inline">\(S^2_{x}=\sum_{i=1}^m(x_i-\bar{x})^2\)</span>.</p>
<p>Ragionando come fatto nel capitolo precedente per la media possiamo verificare la significatività dei coefficienti al <span class="math inline">\(1-\alpha\%\)</span> (<span class="math inline">\(\alpha=0.05\)</span> in generale):</p>
<ul>
<li>test ipotesi (<em>t-test</em>)</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
H_0: \beta_0=0 \quad &amp; \rm{vs} &amp; \quad H_0: \beta_0\neq0 \\
H_0: \beta_1=0 \quad &amp; \rm{vs} &amp; \quad H_1: \beta_0\neq0
\end{eqnarray*}\]</span></p>
<ul>
<li>intervallo di confidenza
<span class="math display">\[\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
\hat\beta_0 &amp; \pm &amp; t(\alpha/2,m-2)s\sqrt{h} \\
\hat\beta_1 &amp; \pm &amp; t(\alpha/2,m-2)s\sqrt{1/S^2_{x}}
\end{eqnarray*}\]</span></li>
</ul>
<p>Sempre dal menù Regressione lineare/semplice otteniamo gli estremi degli intervalli di confidenza al 95%</p>
<p><img src="Immagini/Regressione/06_stima_interv.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Se l’intervallo di confidenza contiene lo 0 non possiamo affermare che il parametro è significativamente diverso da 0, nel caso contrario possiamo affermare che il parametro è significativo. Nell’esempio considerato, sia l’intercetta che la pendenza sono significativamente diversi da 0.</p>
<p>Nella stessa pagina abbiamo anche il seguente output</p>
<p><img src="Immagini/Regressione/07_R_output.png" width="50%" style="display: block; margin: auto;" />
in cui sono riportati oltre al valore dei residui <span class="math inline">\(y_i-(\hat{\beta_0}+\hat{\beta_1}x_i)\)</span>, la stima dei parametri <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span>, il loro errore standard <span class="math inline">\(s\sqrt{h}\)</span> e <span class="math inline">\(s\sqrt{1/S^2_{x}}\)</span> rispettivamente, le statistiche <span class="math inline">\(t\)</span>
<span class="math display">\[
\frac{\hat{\beta_0}}{s\sqrt{h}}, \qquad \qquad \frac{\hat{\beta_1}}{s\sqrt{1/S^2_{x}}}
\]</span></p>
<p>e il <em>p-value</em> dei <em>t-test</em> (vedi il capitolo precedente per la definizione).</p>
<p>Nell’output nella figura precedente viene riportata la stima di <span class="math inline">\(\sigma\)</span> <em>Residual standard error</em> con i relativi gradi di libertà. E’ riportato anche il valore di <span class="math inline">\(R^2\)</span>, un indicatore che misura quanto la variazione di <span class="math inline">\(y\)</span> è descritta dalla variazione di <span class="math inline">\(x\)</span> attraverso la relazione funzionale studiata, (<span class="math inline">\(R^2\)</span> è un indice di <em>fitting</em>, adattamento).</p>
</div>
<div id="previsione-del-modello" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Previsione del modello</h2>
<p>In questo paragrafo ci occupiamo della stima dell’assorbanza “vera” <span class="math inline">\(y_0=\beta_0+\beta_1x_0\)</span> per un valore di concentrazione <span class="math inline">\(x_0\)</span>.</p>
<p><img src="Immagini/Regressione/08_prev.png" width="50%" style="display: block; margin: auto;" /></p>
<p>La stima puntuale della previsione è data da:
<span class="math display">\[
\hat{y_0}=\hat{\beta_0}+\hat{\beta_1}x_0
\]</span>
Il valore di tale stima dipende ovviamente dal campione di 5 misure; un nuovo campione porta ad un valore diverso della stima ma questa, “in media”, ripetendo un gran numero di esperimenti, tende al valore “vero” <span class="math inline">\(y_0=\beta_0+\beta_1x_0\)</span>.</p>
<p><span class="math display">\[
\mathbb{E}(\hat{y_0})=y_0
\]</span></p>
<p>Si osservi che il “valore vero” non è il valore di una misura in <span class="math inline">\(x_0\)</span> che è affetta da errore, ma la “media” di un gran numero di misure in <span class="math inline">\(x_0\)</span>.</p>
<p>E’ nota la precisione dello stimatore <span class="math inline">\(\hat{y_0}\)</span>:
<span class="math display">\[
\rm{Var}(\hat{y_0})=[\frac{1}{m}+\frac{(x_0-\bar{x})^2}{S^2_{x}}]\sigma^2
\]</span>
Il valore <span class="math inline">\(h_0=\frac{1}{m}+\frac{(x_0-\bar{x})^2}{S^2_{x}}\)</span> è chiamato valore leva (<em>leverage</em>) per il punto <span class="math inline">\(x_0\)</span> ed è un indicatore della qualità dello stimatore <span class="math inline">\(\hat{y_0}\)</span>.
L’unico parametro controllabile da cui dipende la precisione della previsione è <span class="math inline">\(S^2_{x}\)</span>. Più grande è tale valore migliori sono le stime.
Poiché <span class="math inline">\(S^2_{x}\)</span> misura lo “spread” delle <span class="math inline">\(x\)</span>, è una buona idea quando si progetta un esperimento allargare il range delle <span class="math inline">\(x\)</span> il più possibile. Inoltre aumentando i valori delle <span class="math inline">\(x\)</span> verso gli estremi del range, si aumenta ulteriormente <span class="math inline">\(S^2_{x}\)</span>.</p>
<p>In figura è indicato con la linea rossa il leverage per ogni valore di concentrazione nel range del nostro esempio, mentre è indicato con una linea verde il leverage con un “disegno sperimentale” leggermente modificato aumentando lo “spread” delle <span class="math inline">\(x\)</span> (i 5 campioni sono per le seguenti concentrazioni: 100, 100, 50, 20, 20)</p>
<p><img src="Immagini/Regressione/11_lev.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Si noti inoltre che a differenza dei modelli per la media visti nel capitolo precedente in cui il valore “leva” dipendeva dal numero di osservazioni nel punto, per la regressione lineare dipende anche dalla “posizione” del punto rispetto alla media.</p>
<p>Altra importante osservazione da fare è che il leverage non dipende dai risultati sperimentali ma dipende soltanto da come sono stati progettati gli esperimenti.</p>
<p>Si può dimostrare che
<span class="math display">\[
\frac{\hat{y_0}-y_0}{s\sqrt{\frac{1}{m}+\frac{(x_0-\bar{x})^2}{S^2_{x}}}}\sim t(m-2).
\]</span></p>
<p>Ragionando come in precedenza possiamo quindi definire il seguente <em>intervallo di confidenza</em> (stima per intervallo della previsione)
<span class="math display">\[
\hat{y_0}\pm t(\alpha/2,m-2)s\sqrt{\frac{1}{m}+\frac{(x_0-\bar{x})^2}{S_{xx}}}
\]</span></p>
<p>Nella seguente figura (ottenuta in Regressione lineare/semplice) è rappresentato l’intervallo di confidenza (zona grigia) per i valori di concentrazione nel range del nostro esempio.</p>
<p><img src="Immagini/Regressione/09_int_conf_prev.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Si noti che, per quanto osservato in precedenza sul leverage, l’intervallo è “più stretto” nella parte centrale del range e quindi la “qualità” della previsione è migliore in quella regione del range.</p>
<p>In Regressione lineare/semplice, fissato il valore <span class="math inline">\(x_0\)</span> della concentrazione viene fornito l’intervallo di confidenza (al 95%) della previsone dell’assorbanza</p>
<p><img src="Immagini/Regressione/10_int_conf_prev_numerico.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="verifica-delle-ipotesi-1" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Verifica delle ipotesi</h2>
<p>Affinché quanto detto sia valido, in particolare per quanto riguarda l’inferenza sui parametri e sulla stima, dobbiamo verificare che siano soddisfatte le ipotesi formulate all’inizio di questo capitolo: tali ipotesi erano che i residui sulle y fossero distribuiti secondo la normale (di media 0), che avessero varianze non troppo differenti tra loro (omoschedasticità) e che fossero indipendenti tra loro.</p>
<p>Nel menù Regressione lineare/semplice alla pagina <em>Verifica ipotesi</em> sono proposti i seguenti test e grafici:</p>
<ul>
<li><strong>Media nulla</strong><br />
sono proposti un <em>t-test</em> in cui l’ipotesi nulla è che la media dei residui sia 0 (la media non è significativamente diversa da 0 per valori del <em>p-value</em> più grandi di <span class="math inline">\(\alpha\)</span> = 0.05) e un grafico dei residui vs i valori predetti (in cui i residui devono essere distribuiti casualmente intorno alla retta orizzontale tratteggiata per convalidare l’ipotesi di indipendenza). Da questo grafico si possono osservare eventuali deviazioni dalla linearità del modello. Se la nuvola dei punti plottati assume qualche particolare forma, ad esempio di parabola, si ha la prova della esistenza di deviazione dalla linearità.</li>
</ul>
<p><img src="Immagini/Regressione/12_ipotesi1.png" width="50%" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Normalità</strong><br />
Viene proposto il <em>test di Shapiro Wilk</em>, uno dei test più potenti per la normalità soprattutto per piccoli campioni: l’ipotesi nulla è che la distribuzione sia normale. La normalità è verificata per valori grandi del <em>p-value</em> (<span class="math inline">\(&gt; \alpha = 0.05\)</span>). Si calcola la statistica <em>W</em> (<span class="math inline">\(0&lt;W&lt;1\)</span>), data dal rapporto di due stimatori alternativi di <span class="math inline">\(\sigma^2\)</span>. Se la distribuzione è normale la statistica W risulta prossima a 1. Viene anche proposto il <em>QQ-Plot</em> in cui sono rappresentati i quantili del campione vs i quantili teorici di una distribuzione normale. Se la distribuzione dei dati del campione è normale i quantili campionari sono avviluppati intorno alla retta tratteggiata.</li>
</ul>
<p><img src="Immagini/Regressione/13_ipotesi2.png" width="50%" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Omoschedasticità</strong><br />
Viene proposto il <em>test di Breusch Pagan</em> la cui ipotesi nulla è l’omoschedasticità dei residui. La omoschedasticità è verificata per valori grandi del <em>p-value</em> (<span class="math inline">\(&gt;\alpha = 0.05\)</span>)). E’ anche proposto un grafico della radice dei residui vs i valori predetti.
In caso di eteroschedasticità nel grafico risulta evidente una tendenza di dispersione, ad esempio crescente, con la nuvola dei punti che assume una forma a cono, cosa che non accade in caso di omoschedasticità degli errori.</li>
</ul>
<p><img src="Immagini/Regressione/14_ipotesi3.png" width="50%" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Correlazione seriale</strong><br />
Viene proposto il <em>test di Durbin-Watson</em> per valutare la presenza di autocorrelazione tra i dati. L’ipotesi nulla è la assenza di autocorrelazione dei residui. La assenza di autocorrelazione è verificata per valori grandi del <em>p-value</em> (<span class="math inline">\(&gt; \alpha = 0.05\)</span>). Il grafico che è stampato insieme alla statistica del test mostra il residuo n-esimo vs il residuo (n-1)-esimo e serve a mettere in evidenza la eventuale correlazione seriale degli errori.</li>
</ul>
<p><img src="Immagini/Regressione/15_ipotesi4.png" width="50%" style="display: block; margin: auto;" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistica-inferenziale.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="analisi-della-varianza.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["dispense_modunivar.pdf"],
"toc": {
"collapse": "section"
},
"info": false,
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
